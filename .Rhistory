debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
source('utilities.R')
getwd()
source('tests/testthst/utilities.R')
source('/tests/testthst/utilities.R')
source('/Users/zukow/Documents/GitHub/RHiveS2/tests/testthst/utilities.R')
library(dplyr)
source('/Users/zukow/Documents/GitHub/RHiveS2/tests/testthat/utilities.R')
source('/tests/testthat/utilities.R')
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
tbl <- tryCatch(dplyr::tbl(sc, name), error = identity)
if (inherits(tbl, "error")) {
if (is.null(data)) data <- eval(as.name(name), envir = parent.frame())
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
}
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
library(dplyr)
library(dbplyr)
library(testthat)
library(RHiveS2)
library(RJDBC)
source('/Users/zukow/Documents/GitHub/RHiveS2/tests/testthat/utilities.R')
name = "iris"
data = NULL
repartition = 0L
sc <- HiveS2_TestConnection()
dplyr::tbl(sc, name)
tbl <- tryCatch(dplyr::tbl(sc, name), error = identity)
if (inherits(tbl, "error")) {
if (is.null(data)) data <- eval(as.name(name), envir = parent.frame())
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
}
tbl
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
date_value <- structure(17455, class = "Date")
show(conn)
dbDisconnect(conn)
conn <- HiveS2_TestConnection()
source('utilities.R')
conn <- HiveS2_TestConnection()
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "so_survey_2019_hive"
)
#' @import RJDBC
import RJDBC
library(RJDBC)
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "so_survey_2019_hive"
)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "so_survey_2019_hive"
)
show(conn)
dbDisconnect(conn)
show(conn)
dbGetQuery(conn, "select * from foo")
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
show(conn)
conn<-NULL
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
usethis::use_test("dbListTables")
dbListTables(conn)
conn <- HiveS2_TestConnection()
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbListTables(conn)
dbListTables(conn, "so_survey_2019_hive")
dbQuoteIdentifier(ANSI(), character()), SQL(character())
equal(dbQuoteIdentifier(ANSI(), character()), SQL(character()))
all.equal(dbQuoteIdentifier(ANSI(), character()), SQL(character()))
equal(dbQuoteIdentifier(ANSI(), "a"), SQL('"a"'))
all.equal(dbQuoteIdentifier(ANSI(), "a"), SQL('"a"'))
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "so_survey_2019_hive"
)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
a<- dbQuoteIdentifier(conn,"a")
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
dbListTables(conn)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
dbListTables(conn)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
traceback()
survey_results_schema_view<-tbl(conn, "survey_results_schema")%>%collect()
name = "mtcars"
data = NULL
repartition = 0L
sc <- conn
#dplyr::tbl(sc, name)
tbl <- tryCatch(dplyr::tbl(sc, name), error = identity)
if (inherits(tbl, "error")) {
if (is.null(data)) data <- eval(as.name(name), envir = parent.frame())
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
}
traceback()
(dbExistsTable(conn, name))
library(dplyr)
library(dbplyr)
library(testthat)
library(RHiveS2)
library(RJDBC)
source('/Users/zukow/Documents/GitHub/RHiveS2/tests/testthat/utilities.R')
name = "iris"
data = NULL
repartition = 0L
sc <- HiveS2_TestConnection()
#dplyr::tbl(sc, name)
tbl <- tryCatch(dplyr::tbl(sc, name), error = identity)
if (inherits(tbl, "error")) {
if (is.null(data)) data <- eval(as.name(name), envir = parent.frame())
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
}
dbWriteTable(c, "iris", iris, overwrite=TRUE)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
dbWriteTable(conn, "iris", iris)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
traceback()
debugSource('~/Documents/GitHub/RHiveS2/R/sanbox-2.R', echo=TRUE)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/R/sanbox-2.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
force(name)
force(max.batch)
force(name)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, append=TRUE)
traceback()
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, append=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
devtools::build()
devtools::load_all()
getwd()
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, append=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/sandbox.R', echo=TRUE)
View(value)
force(statement)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, append=TRUE)
force(statement)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, append=TRUE)
force(statement)
force(object)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbSendUpdate(c, "DROP TABLE IF EXISTS foo")
c<-conn
dbSendUpdate(c, "DROP TABLE IF EXISTS foo")
dbSendUpdate(c, "CREATE TABLE foo (alpha VARCHAR(32), beta INT)")
## prepared update
dbSendUpdate(c, "INSERT INTO foo VALUES (?, ?)", "foo", 123)
dbSendUpdate(c, "INSERT INTO foo VALUES (?, ?)", "bar", 456)
dbWriteTable(conn, "iris", iris, append=TRUE)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
force(list)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
force(x)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
dbWriteTable(conn, "iris", iris)
dbWriteTable(conn, "iris", iris, append=TRUE)
force(list)
View(list)
force(statement)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
## prepared update
dbSendUpdate(c, "INSERT INTO foo VALUES (?, ?)", "foo", 124)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
## prepared update
dbSendUpdate(c, "INSERT INTO foo VALUES (?, ?)", "foo", 124)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, append=TRUE)
force(max.batch)
View(bx)
show(bx@jobj)
force(call)
View(x)
force(x)
View(x)
View(x)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, append=TRUE)
dbWriteTable(conn, "iris", iris, append=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
## prepared update
dbSendUpdate(c, "INSERT INTO foo VALUES (?, ?)", "foo", 128)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
## prepared update
dbSendUpdate(c, "INSERT INTO foo VALUES (?, ?)", "foo", 128)
force(statement)
#' HiveS2Connection class connection class.
#' inherits from JDBCConnection (RJDBC)
#' @import rJava DBI
#' @include dbplyr db_copy_to
#'
#' @export
#' @keywords internal
setClass("HiveS2Connection",
contains = "JDBCConnection",
slots = list(
host = "character",
port = "character",
schema_name = "character",
username = "character"
))
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
rm(list = c(".__C__HiveS2Connection", "tbl"))
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
traceback()
devtools::document()
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
rm(list = c("tbl"))
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
source('/Users/zukow/Documents/GitHub/RHiveS2/tests/testthat/utilities.R')
library(dplyr)
library(dbplyr)
library(testthat)
library(RHiveS2)
library(RJDBC)
source('/Users/zukow/Documents/GitHub/RHiveS2/tests/testthat/utilities.R')
name = "iris"
data = NULL
repartition = 0L
sc <- HiveS2_TestConnection()
#dplyr::tbl(sc, name)
tbl <- tryCatch(dplyr::tbl(sc, name), error = identity)
if (inherits(tbl, "error")) {
if (is.null(data)) data <- eval(as.name(name), envir = parent.frame())
tbl <- dplyr::copy_to(sc, data, name = name, repartition = repartition)
}
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
traceback()
library(RHiveS2)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
force(dest)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
devtools::build()
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
detach("package:dplyr", unload = TRUE)
library(dplyr)
library(RHiveS2)
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
devtools::document()
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
debugSource('~/Documents/GitHub/RHiveS2/tests/testthat/test-sparklyr.R', echo=TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbSendQuery(conn, "Create table foo_send (col1 int, col2 int)
insert values(1,2)
insert values(1,2)")
dbSendQuery(conn, "Create table foo_send (col1 int, col2 int)
insert into foo_send values(1,2)
insert into foo_send values(1,2)")
dbSendQuery(conn, "Create table foo_send (col1 int, col2 int);
insert into foo_send values(1,2);
insert into foo_send values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int)
insert into foo_send1 values(1,2);
insert into foo_send1 values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int)
insert into foo_send1 values(1,2) <EOF>
insert into foo_send1 values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int) <EOF>
insert into foo_send1 values(1,2) <EOF>
insert into foo_send1 values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int)
insert into foo_send1 values(1,2)
insert into foo_send1 values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int)
insert into foo_send1 values(1,2) \n
insert into foo_send1 values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int) \n
insert into foo_send1 values(1,2) \n
insert into foo_send1 values(1,2)")
dbSendQuery(conn, "Create table foo_send1 (col1 int, col2 int)
insert into foo_send1 values(1,2), (1,2)")
dbSendQuery(conn, "Create table foo_send5 (col1 int, col2 int)
insert into foo_send5 values(1,2), (1,2)")
dbSendQuery(conn, "Create table default.foo_send5 (col1 int, col2 int)
insert into foo_send5 values(1,2), (1,2)")
dbSendQuery(conn, "Create table foo_send5 (col1 int, col2 int)")
dbSendQuery(conn, "insert into foo_send5 values(1,2), (1,2)")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
devtools::document()
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
dbWriteTable(conn, "iris", iris, overwrite=TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite=TRUE)

mtcars
iris
mf1 <- copy_to(conn, tibble::tibble(x = 1, y = 1, z = 2) %>% group_by(x, y), "df3", overwrite = TRUE )
mf2 <- mf1 %>% summarise(n = n())
eq<-group_vars(mf2)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
mf1 <- copy_to(conn, tibble::tibble(x = 1, y = 1, z = 2) %>% group_by(x, y), "df3", overwrite = TRUE )
mf2 <- mf1 %>% summarise(n = n())
eq<-group_vars(mf2)
mf1 <- copy_to(conn, tibble::tibble(x = 1, y = 1, z = 2) %>% group_by(x, y), "df3", overwrite = TRUE )%>%collect()
View(mf1)
mf2 <- mf1 %>% summarise(n = n())%>%collect()
View(mf2)
show_query(mf2)
mf2 <- mf1 %>% summarise(n = n())
show_query(mf2)
mf2 <- mf1 %>% summarise(n = n())%>%collect()
eq<-group_vars(mf2)
show_query(mf2)
show_query(mf1)
eq<-group_vars(mf2)
show_query(eq)
mf1 <- copy_to(conn, tibble::tibble(x = 1, y = 1, z = 2) %>% group_by(x, y), "df3", overwrite = TRUE )
show_query(mf1)
mf2 <- mf1 %>% summarise(n = n())
show_query(mf2)
eq<-group_vars(mf2)
show_query(eq)
mf1 <- tibble::tibble(x = 1, y = 1, z = 2) %>% group_by(x, y)
show_query(mf1)
mf2 <- mf1 %>% summarise(n = n())
show_query(mf2)
out <- copy_to(conn, tibble::tibble(x = "a"), "df3", overwrite = TRUE )%>%
group_by(x) %>%
summarise(n = n())
e<-out%>%collect()
q<-tibble::tibble(x = "a", n = 1L)
View(e)
View(q)
View(e)
out <- copy_to(conn, tibble::tibble(x = "a", y="b"), "df3", overwrite = TRUE )%>%
group_by(x) %>%
summarise(n = n())
e<-out%>%collect()
out <- copy_to(conn, tibble::tibble(x = "a"), "df3", overwrite = TRUE )%>%
group_by(x) %>%
summarise(n = n())
show_query(out)
install.packages("dbplyr")
remove.packages("dbplyr")
install_version("dbplyr", version = "1.4.4", repos = "http://cran.us.r-project.org")
devtools::install_version("dbplyr", version = "1.4.4", repos = "http://cran.us.r-project.org")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
mf1 <- copy_to(conn, tibble::tibble(x = 1), "df3", overwrite = TRUE )
val <- 1
mf2 <- mf1 %>% summarise(y = x == val) %>% collect()
View(mf2)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbRemoveTable(conn, "df1")
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbRemoveTable(conn, "df1")
dbRemoveTable(conn, "df1")
dbRemoveTable(conn, "df2")
devtools::document()
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbRemoveTable(conn, "df3")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
library(RHiveS2)
devtools::document()
devtools::document()
devtools::document()
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbListTables(conn)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
library(RHiveS2)
#devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
df1_df <- tibble(a = letters[20:22], b = letters[1:3])
df2_df <- tibble(b = letters[1:3], c = letters[24:26])
df1 <- copy_to(conn, df1_df, "df1",  overwrite = TRUE )
df2 <- copy_to(conn, df2_df, "df2",  overwrite = TRUE )
show_query(left_join(df2, df1) %>% dplyr::arrange(b))
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
use_r("HiveS2Result")
usethis::use_r("HiveS2Result")
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
library(RHiveS2)
#devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbListTables(conn)
sql_query_explain(conn, as.sql("select * from airlines"))
dbplyr::sql_query_explain(conn, as.sql("select * from airlines"))
l<-tbl(conn, "airlines")
q<-show_query(l)
e<-explain(q)
dbplyr:::db_query_fields
dbplyr:::db_query_fields()
db_query_fields.DBIConnection
showMethods("DBIConnection")
showMethods("db_query_fields")
showMethods("db_query_fields()")
trace("db_query_fields")
trace("sql_select")
library(dbplyr)
trace("sql_select")
showMethods("sql_select")
escape.ident
showMethods("escape")
methods(escape)
dbplyr:::escape.ident
showMethods("ident")
dbplyr:::sql_escape_ident.DBIConnection
DBI:::dbQuoteIdentifier.DBIConnection
showMethods("DBIConnection")
methods(class = "DBIConnection")
library(DBI)
DBI:::dbQuoteIdentifier.DBIConnection
DBI:::dbQuoteIdentifier
showMethods("dbQuoteIdentifier")
showMethods(classes = c(conn="DBIConnection", x="character"))
showMethods(dbQuoteIdentifier, classes = c(conn="DBIConnection", x="character"))
getMethod(dbQuoteIdentifier, c(conn="DBIConnection", x="character"))
versionInfo()
versionInfo
versionInfo()
version
devtools::install_version("dbplyr", version = "1.4.4", repos = "http://cran.us.r-project.org", upgrade_dependencies=FALSE)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbListTables(conn)
dbWriteTable(conn, "iris", iris)
dbListTables(conn)
dbSendQuery(conn, "select * from iris")
t<-dbSendQuery(conn, "select * from iris")
dbGetQuery(conn, "select * from iris")
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris)
Q
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
library(dplyr)
library(rJava)
library(RJDBC)
library(dbplyr)
#library(RHiveS2)
devtools::load_all("/Users/zukow/Documents/GitHub/RHiveS2")
options(java.parameters = "-Xmx8g")
#dplyr_spark_connection
#
cp=c("/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar")
.jinit(classpath=cp)
# drv <- JDBC("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
# conn <- DBI::dbConnect(drv,"jdbc:hive2://localhost:10000","so_survey_2019_hive")
#drv <- HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2-standalone.jar",identifier.quote='`')
conn <- DBI::dbConnect(HiveS2("org.apache.hive.jdbc.HiveDriver","/Users/zukow/spark-2.2.1-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar",identifier.quote='`'),
host ="jdbc:hive2://localhost:",
port = "10000",
schema = "default"
)
dbWriteTable(conn, "iris", iris, overwrite = TRUE)
